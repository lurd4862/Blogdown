<!DOCTYPE html>
<html lang="en" class="wf-firasans-n4-active wf-active">
	<head>
    <link href="http://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <!-- Enable responsiveness on mobile devices --> 
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    
    	
    <meta name="generator" content="Hugo 0.37.1" />
    
    <title>Design and Analysis of Experiments with R &middot; </title>
    <meta content="Design and Analysis of Experiments with R - " property="og:title">
    <meta content=" - " property="og:description">    
    <!-- CSS --> 
    <link rel="stylesheet" href="../../css/print.css" media="print">
    <link rel="stylesheet" href="../../css/poole.css">
    <link rel="stylesheet" href="../../css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Sans:300,300i,400,400i,500">
    
    <script defer src="https://use.fontawesome.com/releases/v5.0.6/js/all.js"></script>
    <!-- highlight.js--> 
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <!-- Customised CSS -->
    <link rel="stylesheet" href="../../css/custom.css">
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="../../apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="../../favicon.png">
    

	</head>
    <body>
        <div class="sidebar">
	<div class="container text-center sidebar-sticky">
		<div class="sidebar-about text-center">
			<a href="../../"><h1 class="brand">StatsLab</h1></a>
			 <img src="../../img/logo.jpg" alt="Author Image" class="img-circle headshot center"> 
			<p class="lead">
				 Stefan Fouchè <br> Data Scientist at <br> <a href="http://www.eighty20.co.za/"> Eighty20</a> <br> Loves R and Python 
			</p>
		</div>
		
<div>
	<ul class="sidebar-nav">
		
		
				<li>
					<a href="../../posts/"> <span>Posts</span></a>
				</li>
				<li>
					<a href="../../about_me/"> <span>About</span></a>
				</li>
				<li>
					<a href="../../tags/"> <span>Search by Tag</span></a>
				</li>
		</li>
	</ul>
</div>

        <p>
		<section class="row text-center">
	
	
	
	&nbsp;<a href="https://github.com/lurd4862"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	&nbsp;<a href="https://linkedin.com/in/stefan-fouch%c3%a8-8a126711a"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	&nbsp;<a href="mailto:stefan.fouche.ltd@gmail.com?"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

        </p>
		<p class="copyright">&copy; 2018 Stefan.
        <a href="https://creativecommons.org/licenses/by/4.0">Some Rights Reserved</a>.<br/>Built with <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/htr3n/hyde-hyde">hyde-hyde</a>.
        </p>
	</div>
	<div>
	</div>
</div>

        <div class="content container">
            <div class="post">
  <h1>Design and Analysis of Experiments with R</h1>
  
  <div class="col-sm-12 col-md-12">
    <span class="text-left post-date meta">
            
       <i class="fa fa-calendar" aria-hidden="true"></i> Nov 11, 2017 
      
      
      
      
      </span>  
  </div>    
  
  <div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This document will cover in depth the use of DoE experiments in R! We will focus our attention to measuring treatment effects, not variances.</p>
<p>Based on the work:<br />
<em>Lawson, John. Design and Analysis of Experiments with R. Chapman and Hall/CRC, 20141217. VitalBook file.</em></p>
<div class="figure">
<img src="../../Pictures/Intro.png" />

</div>
</div>
<div id="definitions" class="section level1">
<h1>Definitions</h1>
<ul>
<li><p>Experiment (also called a Run) is an action where the experimenter changes at least one of the variables being studied and then observes the effect of his or her actions(s). Note the passive collection of observational data is not experimentation.</p></li>
<li><p>Experimental Unit is the item under study upon which something is changed. This could be raw materials, human subjects, or just a point in time.</p></li>
<li><p>Sub-Sample, Sub-Unit, or Observational Unit When the experimental unit is split, after the action has been taken upon it, this is called a sub-sample or sub-unit. Sometimes it is only possible to measure a characteristic separately for each sub-unit; for that reason they are often called observational units. Measurements on sub-samples, or sub-units of the same experimental unit, are usually correlated and should be averaged before analysis of data rather than being treated as independent outcomes. When sub-units can be considered independent and there is interest in determining the variance in sub-sample measurements, while not confusing the F-tests on the treatment factors, the mixed model described in Section 5.8 should be used instead of simply averaging the sub-samples.</p></li>
<li><p>Independent Variable (Factor or Treatment Factor) is one of the variables under study that is being controlled at or near some target value, or level, during any given experiment. The level is being changed in some systematic way from run to run in order to determine what effect it has on the response(s).</p></li>
<li><p>Background Variable (also called a Lurking Variable) is a variable that the experimenter is unaware of or cannot control, and which could have an effect on the outcome of the experiment. In a well-planned experimental design, the effect of these lurking variables should balance out so as to not alter the conclusion of a study.</p></li>
<li><p>Dependent Variable (or the Response denoted by Y) is the characteristic of the experimental unit that is measured after each experiment or run. The magnitude of the response depends upon the settings of the independent variables or factors and lurking variables.</p></li>
<li><p>Effect is the change in the response that is caused by a change in a factor or independent variable. After the runs in an experimental design are conducted, the effect can be estimated by calculating it from the observed response data. This estimate is called the calculated effect. Before the experiments are ever conducted, the researcher may know how large the effect should be to have practical importance. This is called a practical effect or the size of a practical effect.</p></li>
<li><p>Replicate runs are two or more experiments conducted with the same settings of the factors or independent variables, but using different experimental units. The measured dependent variable may differ among replicate runs due to changes in lurking variables and inherent differences in experimental units.</p></li>
<li><p>Duplicates refer to duplicate measurements of the same experimental unit from one run or experiment. The measured dependent variable may vary among duplicates due to measurement error, but in the analysis of data these duplicate measurements should be averaged and not treated as separate responses.</p></li>
<li><p>Experimental Design is a collection of experiments or runs that is planned in advance of the actual execution. The particular runs selected in an experimental design will depend upon the purpose of the design.</p></li>
<li><p>Confounded Factors arise when each change an experimenter makes for one factor, between runs, is coupled with an identical change to another factor. In this situation it is impossible to determine which factor causes any observed changes in the response or dependent variable.</p></li>
<li><p>Biased Factor results when an experimenter makes changes to an independent variable at the precise time when changes in background or lurking variables occur. When a factor is biased it is impossible to determine if the resulting changes to the response were caused by changes in the factor or by changes in other background or lurking variables.</p></li>
<li><p>Experimental Error is the difference between the observed response for a particular experiment and the long run average of all experiments conducted at the same settings of the independent variables or factors. The fact that it is called “error” should not lead one to assume that it is a mistake or blunder. Experimental errors are not all equal to zero because background or lurking variables cause them to change from run to run. Experimental errors can be broadly classified into two types: bias error and random error. Bias error tends to remain constant or change in a consistent pattern over the runs in an experimental design, while random error changes from one experiment to another in an unpredictable manner and average to be zero. The variance of random experimental errors can be obtained by including replicate runs in an experimental design.</p></li>
</ul>
</div>
<div id="condensed-theory" class="section level1">
<h1>Condensed theory</h1>
<p>DoE is used to distinguish between correlation and causality in a way that is cost efficient and accounts for most confounding effects and lurker effects.</p>
<p>Sequential strategies are used to determine what factors are most influencial and after that the changes in factors are quantified until finally optimal operating conditions are determined (most appropriate factor states to accomplish maximum treatment effect)</p>
<p><em>DoE Workflow</em></p>
<p><img src="../../Pictures/DoE_workflow.PNG" /> An effective experimental design will ensure the following:</p>
<ul>
<li>A <em>clear objective</em><br />
</li>
<li>An <em>appropriate design</em> plan that gaurantees unconfounded and unbiased factor effects<br />
</li>
<li>A plan for <em>data collection</em> to enable estimation of variance and experimental error</li>
<li>A stipulation to collect enough data to satisfy the objectives</li>
</ul>
<p>In order to achieve this we must:</p>
<ol style="list-style-type: decimal">
<li><em>Define objectives</em><br />
</li>
<li><em>identify experimental units</em><br />
</li>
<li><em>Define a meaningful and measurable respnse or dependent variable</em><br />
</li>
<li><em>List the independent and lurking vbariables</em><br />
</li>
<li><em>Run pilot tests</em><br />
</li>
<li><em>Make a flow diagram of the experimental procedure</em><br />
</li>
<li><em>Choose the experimental design</em><br />
</li>
<li><em>Determine number of replicants</em><br />
</li>
<li><em>Randomize the experimental conditions to experimental units</em><br />
</li>
<li><em>Describe a method for data analysis with tested dummy data</em><br />
</li>
<li><em>Timetable and budget for resources</em></li>
</ol>
<p>In particular it’s important to note that the time we have to run the experiment and the cost budget for the experiment will heavily impact the type of experimental design</p>
</div>
<div id="when-to-use-which-design" class="section level1">
<h1>When to use which design</h1>
<p>Let’s summarise the theory to understand when we should use which experimental design:</p>
<div class="figure">
<img src="../../Pictures/Which_design.PNG" />

</div>
<p>Clearly our very first split depends on what we wish to investigate, the variances or the effects of different factors.</p>
<p><strong>Design Name Acronym Index</strong></p>
<p>RSE - random sampling experiment</p>
<p>FRSE - factorial random sampling experiment</p>
<p>NSE - nested sampling experiment</p>
<p>SNSE - staggered nested sampling experiment</p>
<p>CRD - completely randomized design</p>
<p>CRFD - completely randomized factorial design</p>
<p>CRFF - completely randomized fractional factorial</p>
<p>PB - Plackett-Burman design</p>
<p>OA - orthogonal array design</p>
<p>CRSP - completely randomized split plot</p>
<p>RSSP - response surface split plot</p>
<p>EESPRS - equivalent estimation split-plot response surface</p>
<p>SLD - simplex lattice design</p>
<p>SCD - simplex centroid design</p>
<p>EVD - extreme vertices design</p>
<p>SPMPV - split-plot mixture process variable design</p>
<p>RCB - randomized complete block</p>
<p>GCB - generalized complete block</p>
<p>RCBF - randomized complete block factorial</p>
<p>RBSP - randomized block split plot</p>
<p>PBIB - partially balanced incomplete block</p>
<p>BTIB - balanced treatment incomplete block</p>
<p>BIB - balance incomplete block</p>
<p>BRS - blocked response surface</p>
<p>PCBF - partially confounded blocked factorial</p>
<p>CCBF - completely confounded blocked factorial</p>
<p>LSD - Latin-square design</p>
<p>RCD - row-column design</p>
<div id="study-factor-effects" class="section level2">
<h2>Study factor effects</h2>
<p>We start off with the objective of studying factor effects on treatment response. We will illustrate the basic workflows using a simple 1 factor CRD with 1 response.</p>
<div id="completely-randomized-design-crd" class="section level3">
<h3>Completely randomized design (CRD)</h3>
<div class="figure">
<img src="../../Pictures/CRD.PNG" />

</div>
<p>In completely randomized designs we consider only a single factor. For each possible state of this factor we have a treatment or group. The total number of experimental units n is equal to the number of treatments * the number of replications in of each treatment.</p>
<p><strong>Replications are vital!</strong><br />
Without replications we couldn’t tell if the treatment effect was real or due to random manifistation of unmeasured effects.</p>
<p><strong>Remember</strong>:<br />
By randomly assigning treatments and replications we <code>balance out</code> the effects we are not aware of (law of large numbers fighting off lurker effects). We therefore assume any affects we do not account for are equally distributed accross treatments on average.</p>
<p><strong>Sub-samples are not replicates!</strong><br />
Having two independant tasters taste/rate the same cake does not mean you have two replicates of that particular cake’s treatment.</p>
<div id="example" class="section level4">
<h4>Example</h4>
<p><strong>Bread dough experiment</strong></p>
<p>Test bread rise height caused by factor time:</p>
<p>We test 3 values for time with 4 replicates each (12 experimental units of 3 treatments with 4 replicates)</p>
<pre class="r"><code>set.seed(234789)
f &lt;- factor(rep(c(35,40,45),each = 4))
randomized_times &lt;- sample(f,12)
allocation &lt;- 1:12
design &lt;- data.frame(allocated_loaf = allocation, time = randomized_times)
design</code></pre>
<pre><code>##    allocated_loaf time
## 1               1   40
## 2               2   45
## 3               3   35
## 4               4   40
## 5               5   40
## 6               6   45
## 7               7   45
## 8               8   45
## 9               9   35
## 10             10   40
## 11             11   35
## 12             12   35</code></pre>
</div>
<div id="linear-model-used-for-crd-inference" class="section level4">
<h4>Linear model used for CRD inference</h4>
<p>Obviously we are only making inference about the mean effect within each treatment given the replications:<br />
<strong>Cell mean model</strong><br />
<span class="math inline">\(Y_{ij} = \mu_i + \epsilon_{ij}\)</span></p>
<p>A more useful representation:<br />
<strong>Effects model</strong><br />
<span class="math inline">\(Y_{ij} = \mu + \tau_i + \epsilon_{ij}\)</span><br />
This formulation is more useful because <span class="math inline">\(\tau_i\)</span> is interpreted as the difference of the mean within treatment <span class="math inline">\(i\)</span> and the overall average.</p>
<p>If the design is executed properly the log likelihood function will be maximised if we minimise the sum of squares of residuals (linear regression).</p>
<p><strong>BUT</strong> in R the <code>lm()</code> function will omit the first factor <span class="math inline">\(\tau_1\)</span> and derive coefficients relative to <span class="math inline">\(\mu + \tau_1\)</span>:</p>
<!-- First we turn the design into its matrix notation   -->
<pre class="r"><code>bread_example &lt;- daewr::bread

bread_example %&gt;% lm(formula = height~time) %&gt;% summary</code></pre>
<pre><code>## 
## Call:
## lm(formula = height ~ time, data = .)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -1.812 -1.141  0.000  1.266  2.250 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   5.4375     0.7655   7.104 5.65e-05 ***
## time40        2.8125     1.0825   2.598   0.0288 *  
## time45        2.8750     1.0825   2.656   0.0262 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.531 on 9 degrees of freedom
## Multiple R-squared:  0.5056, Adjusted R-squared:  0.3958 
## F-statistic: 4.602 on 2 and 9 DF,  p-value: 0.042</code></pre>
<p>So we can see the lm() function did not derive a coefficient for the time = 35 factor level. In this call the intercept is interpreted as <span class="math inline">\(\mu + \tau_1\)</span> and the other factor levels are all evaluated relative to this intercept:</p>
<div class="figure">
<img src="../../Pictures/lm_coeffiecients.PNG" />

</div>
<p>Obviously from this equation it’s easy to derive e.g. <span class="math inline">\(\mu + \tau_2\)</span> by adding the 1st and second coefficients together…</p>
</div>
<div id="hypothesis-test-for-no-treatment-effect" class="section level4">
<h4>Hypothesis test for No Treatment Effect</h4>
<p>Now let’s test if the treatment (changing the time factor) has any affect on the response variable (the height of the baked bread):</p>
<p><span class="math display">\[\begin{align}
H_0:&amp; \tau_i = \tau_j \  \forall \  {i,j} \\
H_1:&amp; \textrm{atleast 2 } \tau_i \textrm{&#39;s differ} 
\end{align}\]</span></p>
<pre class="r"><code>bread_example %&gt;% aov(formula = height~time) %&gt;% summary</code></pre>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)  
## time         2  21.57  10.786   4.602  0.042 *
## Residuals    9  21.09   2.344                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>So we can see that the p-value here is the same as the p-value in the lm() summary and since it is &lt; 0.05 we can safely reject <span class="math inline">\(H_0\)</span> and say different time factor levels had a significant impact on the response variable height.</p>
</div>
<div id="verify-linearity-assumption" class="section level4">
<h4>Verify linearity assumption</h4>
<p>Of course, using a lm() model assumes linearity and we should validate this assumed relationship between time and height:</p>
</div>
<div id="assumptions-validation-function" class="section level4">
<h4>Assumptions validation function</h4>
<pre class="r"><code>Validate_assumptions &lt;- function(df, formula_ = NULL, model, factor, response) {
  qq_plot &lt;- 
    df %&gt;% 
    ggplot(aes(sample = model %&gt;% residuals))+
    stat_qq()+
    ggtitle(&quot;QQ plot of residuals&quot;)+
    xlab(&quot;&quot;)+
    ylab(&quot;&quot;)
  
  Res_fac &lt;- df %&gt;% 
    ggplot()+
    geom_point(aes(y = model %&gt;% residuals(), x = df[,factor] ))+
    ggtitle(&quot;Residuals VS factor levels&quot;) +
    xlab(&quot;Residuals&quot;)+
    ylab(&quot;Factor levels&quot;)

  Res_fit &lt;- df %&gt;% 
    ggplot(aes(y = model %&gt;% residuals(), x = model %&gt;% predict))+
    geom_point()+
    ggtitle(&quot;Residuals VS fitted&quot;)+
    xlab(&quot;Residuals&quot;)+
    ylab(&quot;fitted&quot;)
  
  Res_eu &lt;- df %&gt;% 
    ggplot(aes(y = model %&gt;% residuals(), x = seq(nrow(df))))+
    geom_point()+
    ggtitle(&quot;Residuals VS experimental unit&quot;)+
    xlab(&quot;Residuals&quot;)+
    ylab(&quot;Experimental unit&quot;)
  
  multiplot &lt;- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots &lt;- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use &#39;cols&#39; to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
  
  multiplot(qq_plot, Res_fac, Res_fit, Res_eu, cols=2)
  
}

Validate_assumptions(df = bread_example,model =  bread_example %&gt;% lm(formula = height~time), factor = &quot;time&quot;) </code></pre>
<p><img src="../../posts/Design_and_Analysis_of_Experiments_with_R_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code># qqnorm(y = bread_example %&gt;% lm(formula = height~time) %&gt;% residuals)</code></pre>
<p>In this case we do seem to have a fairly normal response relationship but with some variability in the lower corner. Consider that we only have 12 points.</p>
<p>A more serious violation of assumption is heterogeneity of variances. In general when the linearity assumption is violated so will the homogeniety of variances assumption. To remedy this we can apply a transformation to the data to reconsile the linearity assumption.</p>
</div>
<div id="box-cox-power-transformations" class="section level4">
<h4>Box-Cox Power Transformations</h4>
<p>** When variation is relative to the mean observation within each treatment **</p>
<p>This transformation will change the data so that it is distributed normally. Normally distributed data has honogeneous variances</p>
<p>These transformations ensure that <span class="math inline">\(\sigma \propto \mu^{1 - \lambda}\)</span></p>
<p>To find <span class="math inline">\(\lambda\)</span> we can do:</p>
<pre class="r"><code>box_cox &lt;- boxcox(aov(bread_example,formula = height~time))</code></pre>
<p><img src="../../posts/Design_and_Analysis_of_Experiments_with_R_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>lambda = box_cox$x[which(box_cox$y == box_cox$y %&gt;% max)]
lambda</code></pre>
<pre><code>## [1] -0.5050505</code></pre>
<p>Now we transform the data:</p>
<pre class="r"><code>bread_example_transformed &lt;- 
  bread_example %&gt;% 
  mutate(height = height^lambda)</code></pre>
<p>Let’s redo the normality validation on residuals:</p>
<pre class="r"><code>qqnorm(y = bread_example_transformed %&gt;% lm(formula = height~time) %&gt;% residuals())</code></pre>
<p><img src="../../posts/Design_and_Analysis_of_Experiments_with_R_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>And if we look at the hypothesis test with the now normally distributed data:</p>
<pre class="r"><code>bread_example_transformed %&gt;% aov(formula = height~time) %&gt;% summary</code></pre>
<pre><code>##             Df  Sum Sq  Mean Sq F value Pr(&gt;F)  
## time         2 0.01732 0.008662   6.134 0.0209 *
## Residuals    9 0.01271 0.001412                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Our new p-value is smaller which indicates that an observation as or more extreme than what we have is even more unlikely under the null hypothesis that there is no treatment effect.</p>
</div>
<div id="distributional-transformations" class="section level4">
<h4>Distributional transformations</h4>
<p>** When variation is relative to the mean observation within each treatment **</p>
<p>An alternative to box-cox power transformation</p>
<div class="figure">
<img src="../../Pictures/Distributional_transformations.png" />

</div>
<p>OK, well let’s use the lognormal transformation and see the effect on the anova:</p>
<pre class="r"><code>bread_example_lognormal &lt;- 
  bread_example %&gt;% 
  mutate(height = log(height))</code></pre>
<p>Let’s redo the normality validation on residuals:</p>
<pre class="r"><code>qqnorm(y = bread_example_lognormal %&gt;% lm(formula = height~time) %&gt;% residuals())</code></pre>
<p><img src="../../posts/Design_and_Analysis_of_Experiments_with_R_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>In this case we seem to fit the tail distribution better…</p>
<p>And if we look at the hypothesis test with the now more? normally distributed errors:</p>
<pre class="r"><code>bread_example_lognormal %&gt;% aov(formula = height~time) %&gt;% summary</code></pre>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)  
## time         2 0.4598 0.22992    5.64 0.0258 *
## Residuals    9 0.3669 0.04077                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The p-value is again shifted towards 0.0258 which indicates with higher certainty that we reject the null hypothesis</p>
</div>
</div>
<div id="alternatives-to-least-squares-regression" class="section level3">
<h3>Alternatives to least squares regression</h3>
<div id="weighted-linear-model" class="section level4">
<h4>Weighted linear model</h4>
<p>When we believe the <strong>heterogeneous variances are not related to the treatment mean</strong> we can use a weighted least squares</p>
<p>Use sd() as weights within treatments:</p>
<pre class="r"><code>bread_example_Weighted &lt;- 
    bread_example %&gt;% 
  group_by(time) %&gt;% 
  mutate(sd_treatment = 1/sd(height))
bread_example_Weighted</code></pre>
<pre><code>## # A tibble: 12 x 4
## # Groups:   time [3]
##     loaf time  height sd_treatment
##    &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;        &lt;dbl&gt;
##  1  1.00 35      4.50        1.04 
##  2  2.00 35      5.00        1.04 
##  3  3.00 35      5.50        1.04 
##  4  4.00 35      6.75        1.04 
##  5  5.00 40      6.50        0.485
##  6  6.00 40      6.50        0.485
##  7  7.00 40     10.5         0.485
##  8  8.00 40      9.50        0.485
##  9  9.00 45      9.75        0.735
## 10 10.0  45      8.75        0.735
## 11 11.0  45      6.50        0.735
## 12 12.0  45      8.25        0.735</code></pre>
<p>Now that we have the weights for each treatment we can use it in a weighted OLS:</p>
<pre class="r"><code>bread_example_Weighted %&gt;% lm(formula = height~time,weights = bread_example_Weighted$sd_treatment) %&gt;% summary</code></pre>
<pre><code>## 
## Call:
## lm(formula = height ~ time, data = ., weights = bread_example_Weighted$sd_treatment)
## 
## Weighted Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.5543 -1.0203  0.0050  0.9611  1.5671 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   5.4375     0.5941   9.152 7.44e-06 ***
## time40        2.8125     1.0520   2.674   0.0255 *  
## time45        2.8750     0.9220   3.118   0.0124 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.209 on 9 degrees of freedom
## Multiple R-squared:  0.5805, Adjusted R-squared:  0.4872 
## F-statistic: 6.226 on 2 and 9 DF,  p-value: 0.02006</code></pre>
<p>This gives us a p-value of 0.02006 which is very similar to the other transformed methods</p>
</div>
<div id="creating-a-weighted-ols-function" class="section level4">
<h4>Creating a weighted OLS function</h4>
<p>So to generalize this for simple case of CRD:</p>
<pre class="r"><code>weighted_ols_anova &lt;-  function(df, formula_, factor, response) {
  factor_enq &lt;- dplyr::enquo(factor)
  response_enq &lt;- dplyr::enquo(response)
  # factor_quo &lt;- dplyr::quo(factor)
  # response_quo &lt;- dplyr::quo(response)
  df &lt;- 
    df %&gt;% 
  group_by(!!factor_enq) %&gt;%
  mutate(sd_treatment = 1/sd(!!response_enq)) %&gt;% 
    ungroup
  
   # y = df %&gt;% select(!!response_enq) %&gt;% flatten_dbl
   # x = df %&gt;% select(!!factor_enq) %&gt;% flatten_dbl
  
    df %&gt;% lm(formula =  formula_, weights = sd_treatment) %&gt;% anova
    # df %&gt;% lm(y = y, x = x, weights = sd_treatment) %&gt;% anova

  # return(df)
}

weighted_ols_anova(bread,height~time,time,height)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: height
##           Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## time       2 18.209  9.1047  6.2263 0.02006 *
## Residuals  9 13.161  1.4623                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="generalized-linear-models" class="section level4">
<h4>Generalized linear models</h4>
<p><strong>Pol data using logistic link function assuming multinomial distribution</strong></p>
<p>Participants were asked to rank a treatment from 1 to 5</p>
<p>Data:</p>
<pre class="r"><code>mod_full &lt;- MASS::polr(data = daewr::teach,
                   formula = score ~ method,
                   weight = count)

mod_reduced &lt;- MASS::polr(data = daewr::teach,
                   formula = score ~ 1,
                   weight = count)

anova(mod_full,mod_reduced)</code></pre>
<pre><code>## Likelihood ratio tests of ordinal regression models
## 
## Response: score
##    Model Resid. df Resid. Dev   Test    Df LR stat.     Pr(Chi)
## 1      1       294   885.9465                                  
## 2 method       292   876.2986 1 vs 2     2  9.64787 0.008035108</code></pre>
<p>The reduced model predicts assuming only a global mean which conceptually means:<br />
<span class="math inline">\(y_{i,j} = \mu + \epsilon_{i,j} \ \ \  VS \ \ \  y_{i,j}=\mu + \tau_i + \epsilon_{i,j}\)</span></p>
<p>This shows that using different methods does make a difference compared to no particular method used. Because the p-value here rejects the null hypothesis that the two models are the same and therefore rejects the null hypothesis that all treatment effects are the same: <span class="math inline">\(\mu_i = \mu_j \  \forall \  {i,j}\)</span></p>
<p><strong>How do we measure treatment effect?</strong></p>
<p>Let’s visualize the scores by method used</p>
<pre class="r"><code># Predicted &lt;- 
#   daewr::teach %&gt;% 
#   mutate(predicted_score = mod_full %&gt;% predict)

# Predicted %&gt;% 
#   group_by(method) %&gt;% 
#   nest %&gt;% 
#   mutate(Histogram = data %&gt;% map(~hist(.$score))) %&gt;% 
#   .Histogram

daewr::teach %&gt;% 
  ggplot()+
  geom_bar(aes(x = score,y = count),stat = &quot;identity&quot;)+
  facet_wrap(~method)+
  ggtitle(&quot;Distribution of scores by method&quot;,subtitle = &quot;Method 3 appears to outperform&quot;)</code></pre>
<p><img src="../../posts/Design_and_Analysis_of_Experiments_with_R_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<pre class="r"><code>daewr::teach %&gt;% 
  mutate_if(is.factor,as.numeric) %&gt;% 
  group_by(method) %&gt;% 
  summarise(mean_score = sum(score*count)/sum(count),
            Total_votes = sum(count))</code></pre>
<pre><code>## # A tibble: 3 x 3
##   method mean_score Total_votes
##    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;
## 1   1.00       2.98        88.0
## 2   2.00       3.22       104  
## 3   3.00       3.47       106</code></pre>
<p>We can see that on average the 3rd treatment here would outperform the rest of the treatment groups from this designed experiment.</p>
<p><strong>Test for assumption violations</strong></p>
<p>We can’t repeat the usual battery of tests here since we are dealing with a categorical response and the idea of residual error should be defined differently from say <span class="math inline">\(y-\hat{y}\)</span>.</p>
<!-- bread_example_lm <- daewr::bread %>% lm(formula = height~time) -->
<!-- Validate_assumptions(df = daewr::bread, -->
<!--                      model = bread_example_lm, -->
<!--                      factor = "time", -->
<!--                      response = "height") -->
<p>In order to run the usual battery of validity tests we need the residuals of the fitted model. However, this is a categorical response experiment.</p>
<pre class="r"><code># Validate_assumptions(df = daewr::teach, model = mod_full)</code></pre>
</div>
<div id="deternining-the-number-of-replicates" class="section level4">
<h4>Deternining the number of replicates</h4>
<p><strong>Bread dough experiment</strong></p>
<p>The idea of <code>practical difference</code> is important here. If the practitioner wants to monitor any difference in <code>cell means</code> larger than say 3 inches he would use the <code>Fpower1</code> function with <span class="math inline">\(\Delta = 3\)</span>.</p>
<p><code>Fpower1</code> calculates the power of the 1 way anova F distribution</p>
<p>Now we specify the number of replications we would like to calculate the power for given we know the variance of the experimental error <span class="math inline">\(\hat{\sigma}^2\)</span></p>
<p>If we assume the naive OLS this value is:</p>
<pre class="r"><code>daewr::bread %&gt;% lm(formula = height~time) %&gt;% residuals() %&gt;% var</code></pre>
<pre><code>## [1] 1.917614</code></pre>
<p>Let’s assume the value <span class="math inline">\(\hat{\sigma}^2 = 1.917614\)</span></p>
<pre class="r"><code>power &lt;- Fpower1(alpha = rep(0.05, 6-2+1),
                 nlev = 3,
                 nreps = 2:6, 
                 Delta = 3,
                 sigma = sqrt(1.917614)
                )
power</code></pre>
<pre><code>##      alpha nlev nreps Delta    sigma     power
## [1,]  0.05    3     2     3 1.384779 0.2086501
## [2,]  0.05    3     3     3 1.384779 0.4366902
## [3,]  0.05    3     4     3 1.384779 0.6315614
## [4,]  0.05    3     5     3 1.384779 0.7730600
## [5,]  0.05    3     6     3 1.384779 0.8666144</code></pre>
<p>This table then shows us that with as many as 6 replicates per treatment we can detect with ~87% accuracy a <code>cell mean</code> shift of 3 inches.</p>
<p>Based on the affordability in terms of time and money we can limit the factor under investigation using screening designs and perhaps limit the number of factors such that we get enough power out of the affordable number of replications.</p>
</div>
<div id="choosing-optimal-treatments" class="section level4">
<h4>Choosing optimal treatments</h4>
<p>In general it is possible to construct hypothesis tests of comparisons before the design of experiments is carried out and responses are recorded but we will not focus on that here.</p>
<p>Instead we will assume the practitioner is interested in comparing treatments to one another in an unbiased way or compared to the current control treatment being used.</p>
</div>
<div id="unplanned-comparisons" class="section level4">
<h4>Unplanned Comparisons</h4>
<p>When comparing specific treatments with one another <em>after</em> the experiment has been designed and carried out we must adjust for bias.</p>
<p>In R we can use the <code>TukeyHSD</code> function to compare cell mean shifts between different treatments</p>
<p><strong>Sugarbeet dataset</strong></p>
<p>This dataset is another CRD design. Let’s illustrate the TukeyHSD function here:</p>
<pre class="r"><code>Sugerbeet_anova &lt;- aov(yield ~ treat, data = daewr::sugarbeet)

Sugerbeet_anova</code></pre>
<pre><code>## Call:
##    aov(formula = yield ~ treat, data = daewr::sugarbeet)
## 
## Terms:
##                    treat Residuals
## Sum of Squares  291.0050   29.5865
## Deg. of Freedom        3        14
## 
## Residual standard error: 1.453727
## Estimated effects may be unbalanced</code></pre>
<pre class="r"><code>Sugerbeet_TukeyHSD &lt;- TukeyHSD(Sugerbeet_anova, ordered = TRUE)

Sugerbeet_TukeyHSD</code></pre>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
##     factor levels have been ordered
## 
## Fit: aov(formula = yield ~ treat, data = daewr::sugarbeet)
## 
## $treat
##     diff        lwr       upr     p adj
## B-A  6.3  3.3122236  9.287776 0.0001366
## D-A 10.0  7.1655464 12.834454 0.0000004
## C-A 10.1  7.2655464 12.934454 0.0000003
## D-B  3.7  0.8655464  6.534454 0.0094231
## C-B  3.8  0.9655464  6.634454 0.0077551
## C-D  0.1 -2.5723484  2.772348 0.9995162</code></pre>
<p>Here we can see that treatments <code>C and D</code> do not reject the null hypothesis that they have different cell means. However, the test also shows that no other treatments are considered the same and all reject at 5%.</p>
<p>Based purely on the confidence interval showed we could conclude that treatments <code>C and D</code> appear to be the most optimal if a higher mean in the response was required.</p>
<p>This function does not explicitly tell us what those means are, merely their differences.</p>
<p><strong>Another, more conservative, function we can use is the <code>SNK.test</code> function from the agricolae package</strong>:</p>
<pre class="r"><code>Sugarbeet_SNK &lt;- SNK.test(Sugerbeet_anova, &quot;treat&quot;, alpha = 0.05)

Sugerbeet_anova</code></pre>
<pre><code>## Call:
##    aov(formula = yield ~ treat, data = daewr::sugarbeet)
## 
## Terms:
##                    treat Residuals
## Sum of Squares  291.0050   29.5865
## Deg. of Freedom        3        14
## 
## Residual standard error: 1.453727
## Estimated effects may be unbalanced</code></pre>
<p>This time we can clearly see the treatment means along with their summary statistics. As for rejection of the null hypothesis we can compare the <code>M</code> column in the groups output. Treatments that did not reject the null would have the same letters here.</p>
<p><strong>Comparison to the current control treatment</strong></p>
<p>To compare different treatments to the current control treatment we can use the function <code>glht</code> from the multcomp package</p>
<p><strong>The glht function assumes that the 1st factor level is in fact the control treatment</strong></p>
<pre class="r"><code>Sugarbeet_glht &lt;- glht(Sugerbeet_anova,linfct = mcp(treat = &quot;Dunnett&quot;), alternative = &quot;greater&quot;)

Sugarbeet_glht %&gt;% summary</code></pre>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Dunnett Contrasts
## 
## 
## Fit: aov(formula = yield ~ treat, data = daewr::sugarbeet)
## 
## Linear Hypotheses:
##            Estimate Std. Error t value Pr(&gt;t)    
## B - A &lt;= 0   6.3000     1.0279   6.129 &lt;1e-04 ***
## C - A &lt;= 0  10.1000     0.9752  10.357 &lt;1e-04 ***
## D - A &lt;= 0  10.0000     0.9752  10.254 &lt;1e-04 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- single-step method)</code></pre>
<p>By specifying <code>alternative = &quot;greater&quot;</code> we can test the null hypothesis of the <code>cell mean</code> being greater than that of the control treatment using the <code>Dunnett</code> method.</p>
<p>Here we can again see that treatments <code>C and D</code> are superior to the control treatment <code>A</code>.</p>
</div>
</div>
</div>
</div>

</div>
            <div class="footer">
                <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>

<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>


        <h2>Comments</h2>
        <div id="disqus_thread"></div>
<script>
(function() {
var d = document, s = d.createElement('script');
s.src = 'https://stats-lab.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    
<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

            </div>
        </div>
        
                
    </body>
</html>
